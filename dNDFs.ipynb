{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Duyih25/Bomberman/blob/main/dNDFs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMVgHSs_n5uv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSO_PDHmqp09",
        "outputId": "1c89e5fd-3921-4075-f093-21dd45fcf1f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZySkxxCnkxd",
        "outputId": "7bb161fc-5136-4a7c-e63e-af2891bc4373"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PRODUCT_CODE      int64\n",
              "AGE               int64\n",
              "PROVINCE          int64\n",
              "TD_2            float64\n",
              "TD_3            float64\n",
              "TD_4            float64\n",
              "DATA_2          float64\n",
              "DATA_3          float64\n",
              "DATA_4          float64\n",
              "ONNET_IN_2      float64\n",
              "ONNET_IN_3      float64\n",
              "ONNET_IN_4      float64\n",
              "ONNET_OUT_2     float64\n",
              "ONNET_OUT_3     float64\n",
              "ONNET_OUT_4     float64\n",
              "OFFNET_IN_2     float64\n",
              "OFFNET_IN_3     float64\n",
              "OFFNET_IN_4     float64\n",
              "OFFNET_OUT_2    float64\n",
              "OFFNET_OUT_3    float64\n",
              "OFFNET_OUT_4    float64\n",
              "PACK_TIME_2       int64\n",
              "PACK_TIME_3       int64\n",
              "PACK_TIME_4       int64\n",
              "PACK_MONEY_2    float64\n",
              "PACK_MONEY_3    float64\n",
              "PACK_MONEY_4    float64\n",
              "RC_TIME_2         int64\n",
              "RC_TIME_3         int64\n",
              "RC_TIME_4         int64\n",
              "RC_MONEY_2        int64\n",
              "RC_MONEY_3        int64\n",
              "RC_MONEY_4        int64\n",
              "REG_ON_2          int64\n",
              "REG_ON_3          int64\n",
              "REG_ON_4          int64\n",
              "15c3d_2           int64\n",
              "15c3d_3           int64\n",
              "15c3d_4           int64\n",
              "15c3d_5          object\n",
              "OS_FF             uint8\n",
              "OS_SM             uint8\n",
              "OS_Unknow         uint8\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/churn/sub_model_t5.csv')\n",
        "\n",
        "data.drop('ISDN', axis= 1, inplace= True)\n",
        "\n",
        "data.drop('TD_5', axis= 1, inplace= True)\n",
        "data.drop('DATA_5', axis= 1, inplace= True)\n",
        "data.drop('ONNET_IN_5', axis= 1, inplace= True)\n",
        "data.drop('ONNET_OUT_5', axis= 1, inplace= True)\n",
        "data.drop('OFFNET_IN_5', axis= 1, inplace= True)\n",
        "data.drop('OFFNET_OUT_5', axis= 1, inplace= True)\n",
        "data.drop('PACK_TIME_5', axis= 1, inplace= True)\n",
        "data.drop('PACK_MONEY_5', axis= 1, inplace= True)\n",
        "data.drop('RC_TIME_5', axis= 1, inplace= True)\n",
        "data.drop('RC_MONEY_5', axis= 1, inplace= True)\n",
        "data.drop('REG_ON_5', axis= 1, inplace= True)\n",
        "\n",
        "# encode data\n",
        "data = pd.get_dummies(data)\n",
        "\n",
        "data['15c3d_5'] = data['15c3d_5'].astype(str)\n",
        "data['15c3d_5'].replace([\"0\", \"1\"], [\"y\", \"n\"], inplace= True)\n",
        "\n",
        "data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9eDqYknU4XD"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YCWoJ5i7rHH"
      },
      "outputs": [],
      "source": [
        "### finding ouliers\n",
        "def outliers(df, ft):\n",
        "    if ft in (\"ISDN\",\"PRODUCT_CODE\",\"AGE\",\"PROVINCE\",\"OS\",\"15c3d_5\"):\n",
        "        return []\n",
        "    \n",
        "    Q1 = df[ft].quantile(0.05)\n",
        "    Q3 = df[ft].quantile(0.95)\n",
        "    IQR = Q3 - Q1\n",
        "    \n",
        "    upper_bound = Q3 + 20 * IQR\n",
        "    return df.index[(df[ft] < 0) | (df[ft] > upper_bound)].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MeP8w6-_qvy",
        "outputId": "cc26f8a5-db6f-436b-efe8-3004eed5850d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127\n"
          ]
        }
      ],
      "source": [
        "index_list = []\n",
        "\n",
        "for col in data:\n",
        "    index_list.extend(outliers(data, col))\n",
        "\n",
        "index_list = list(dict.fromkeys(index_list))    \n",
        "print(len(index_list))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXsG0Tu6_q-R"
      },
      "outputs": [],
      "source": [
        "# remove outliers\n",
        "\n",
        "def remove(df, ls):\n",
        "    ls = sorted(ls)\n",
        "    df = df.drop(ls)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTzzKAE0_rNE"
      },
      "outputs": [],
      "source": [
        "data = remove(data, index_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBGEPnskn3fd"
      },
      "outputs": [],
      "source": [
        "data_train,data_test = train_test_split(data, test_size= 0.2, random_state= 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8YylG1evJov",
        "outputId": "829341d1-d5ee-40e5-bce1-d76312c9b3a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       PRODUCT_CODE  AGE  PROVINCE     TD_2  ...  15c3d_5  OS_FF  OS_SM  OS_Unknow\n",
            "72248      10000022  132  10000015   0.0000  ...        n      0      1          0\n",
            "10124      10000001  932  10000016   3.5268  ...        y      0      1          0\n",
            "89681      10000001  202  10000009   0.1330  ...        n      1      0          0\n",
            "70388      10000013  187  10000008  10.0000  ...        y      1      0          0\n",
            "77474      10000001  369  10000001  40.0000  ...        n      0      0          1\n",
            "\n",
            "[5 rows x 43 columns]\n",
            "       PRODUCT_CODE  AGE  PROVINCE     TD_2  ...  15c3d_5  OS_FF  OS_SM  OS_Unknow\n",
            "36228      10000001   94  10000018   0.0000  ...        n      0      1          0\n",
            "61537      10000001  762  10000025  10.0000  ...        y      0      1          0\n",
            "46150      10000001  797  10000003  21.4303  ...        n      1      0          0\n",
            "13581      10000001  264  10000001   0.0000  ...        y      0      1          0\n",
            "77044      10000001  117  10000015   0.0000  ...        n      0      1          0\n",
            "\n",
            "[5 rows x 43 columns]\n"
          ]
        }
      ],
      "source": [
        "print(data_train.head())\n",
        "print(data_test.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XK4zFc_vJ_T"
      },
      "outputs": [],
      "source": [
        "data_train.to_csv(\"train.csv\", index=False, header=False)\n",
        "data_test.to_csv(\"test.csv\", index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJBxP_8So59n"
      },
      "outputs": [],
      "source": [
        "# A list of the numerical feature names.\n",
        "NUMERIC_FEATURE_NAMES = data.drop(\"15c3d_5\", axis=1).columns\n",
        "# A dictionary of the categorical features and their vocabulary.\n",
        "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
        "    \n",
        "}\n",
        "# A list of the columns to ignore from the dataset.\n",
        "#IGNORE_COLUMN_NAMES = [\"fnlwgt\"]\n",
        "# A list of the categorical feature names.\n",
        "CATEGORICAL_FEATURE_NAMES = []\n",
        "# A list of all the input features.\n",
        "FEATURE_NAMES = NUMERIC_FEATURE_NAMES\n",
        "# A list of column default values for each feature.\n",
        "COLUMN_DEFAULTS = [\n",
        "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES else [\"NA\"]\n",
        "    for feature_name in data.columns\n",
        "]\n",
        "# The name of the target feature.\n",
        "TARGET_FEATURE_NAME = \"15c3d_5\"\n",
        "# A list of the labels of the target features.\n",
        "TARGET_LABELS = [\"y\", \"n\"]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Dmf1V5vo6Mx"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import StringLookup\n",
        "\n",
        "target_label_lookup = StringLookup(\n",
        "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
        ")\n",
        "\n",
        "\n",
        "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_names=data.columns,\n",
        "        column_defaults=COLUMN_DEFAULTS,\n",
        "        label_name=TARGET_FEATURE_NAME,\n",
        "        num_epochs=1,\n",
        "        header=False,\n",
        "        na_value=\" \",\n",
        "        shuffle=shuffle,\n",
        "    ).map(lambda features, target: (features, target_label_lookup(target)))\n",
        "    return dataset.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp-xoHoIo6U5"
      },
      "outputs": [],
      "source": [
        "def create_model_inputs():\n",
        "    inputs = {}\n",
        "    for feature_name in FEATURE_NAMES:\n",
        "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
        "            inputs[feature_name] = layers.Input(\n",
        "                name=feature_name, shape=(), dtype=tf.float32\n",
        "            )\n",
        "        else:\n",
        "            inputs[feature_name] = layers.Input(\n",
        "                name=feature_name, shape=(), dtype=tf.string\n",
        "            )\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI5IyFG0o6Xg"
      },
      "outputs": [],
      "source": [
        "def encode_inputs(inputs):\n",
        "    encoded_features = []\n",
        "    for feature_name in inputs:\n",
        "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
        "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "            # Create a lookup to convert a string values to an integer indices.\n",
        "            # Since we are not using a mask token, nor expecting any out of vocabulary\n",
        "            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
        "            lookup = StringLookup(\n",
        "                vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n",
        "            )\n",
        "            # Convert the string input values into integer indices.\n",
        "            value_index = lookup(inputs[feature_name])\n",
        "            embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n",
        "            # Create an embedding layer with the specified dimensions.\n",
        "            embedding = layers.Embedding(\n",
        "                input_dim=lookup.vocabulary_size(), output_dim=embedding_dims\n",
        "            )\n",
        "            # Convert the index values to embedding representations.\n",
        "            encoded_feature = embedding(value_index)\n",
        "        else:\n",
        "            # Use the numerical features as-is.\n",
        "            encoded_feature = inputs[feature_name]\n",
        "            if inputs[feature_name].shape[-1] is None:\n",
        "                encoded_feature = tf.expand_dims(encoded_feature, -1)\n",
        "\n",
        "        encoded_features.append(encoded_feature)\n",
        "\n",
        "    encoded_features = layers.concatenate(encoded_features)\n",
        "    return encoded_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmrtMdbZo6Zo"
      },
      "outputs": [],
      "source": [
        "class NeuralDecisionTree(keras.Model):\n",
        "    def __init__(self, depth, num_features, used_features_rate, num_classes):\n",
        "        super(NeuralDecisionTree, self).__init__()\n",
        "        self.depth = depth\n",
        "        self.num_leaves = 2 ** depth\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Create a mask for the randomly selected features.\n",
        "        num_used_features = int(num_features * used_features_rate)\n",
        "        one_hot = np.eye(num_features)\n",
        "        sampled_feature_indicies = np.random.choice(\n",
        "            np.arange(num_features), num_used_features, replace=False\n",
        "        )\n",
        "        self.used_features_mask = one_hot[sampled_feature_indicies]\n",
        "\n",
        "        # Initialize the weights of the classes in leaves.\n",
        "        self.pi = tf.Variable(\n",
        "            initial_value=tf.random_normal_initializer()(\n",
        "                shape=[self.num_leaves, self.num_classes]\n",
        "            ),\n",
        "            dtype=\"float32\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        # Initialize the stochastic routing layer.\n",
        "        self.decision_fn = layers.Dense(\n",
        "            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n",
        "        )\n",
        "\n",
        "    def call(self, features):\n",
        "        batch_size = tf.shape(features)[0]\n",
        "\n",
        "        # Apply the feature mask to the input features.\n",
        "        features = tf.matmul(\n",
        "            features, self.used_features_mask, transpose_b=True\n",
        "        )  # [batch_size, num_used_features]\n",
        "        # Compute the routing probabilities.\n",
        "        decisions = tf.expand_dims(\n",
        "            self.decision_fn(features), axis=2\n",
        "        )  # [batch_size, num_leaves, 1]\n",
        "        # Concatenate the routing probabilities with their complements.\n",
        "        decisions = layers.concatenate(\n",
        "            [decisions, 1 - decisions], axis=2\n",
        "        )  # [batch_size, num_leaves, 2]\n",
        "\n",
        "        mu = tf.ones([batch_size, 1, 1])\n",
        "\n",
        "        begin_idx = 1\n",
        "        end_idx = 2\n",
        "        # Traverse the tree in breadth-first order.\n",
        "        for level in range(self.depth):\n",
        "            mu = tf.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]\n",
        "            mu = tf.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n",
        "            level_decisions = decisions[\n",
        "                :, begin_idx:end_idx, :\n",
        "            ]  # [batch_size, 2 ** level, 2]\n",
        "            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n",
        "            begin_idx = end_idx\n",
        "            end_idx = begin_idx + 2 ** (level + 1)\n",
        "\n",
        "        mu = tf.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]\n",
        "        probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]\n",
        "        outputs = tf.matmul(mu, probabilities)  # [batch_size, num_classes]\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8-VMl74o6cH"
      },
      "outputs": [],
      "source": [
        "class NeuralDecisionForest(keras.Model):\n",
        "    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n",
        "        super(NeuralDecisionForest, self).__init__()\n",
        "        self.ensemble = []\n",
        "        # Initialize the ensemble by adding NeuralDecisionTree instances.\n",
        "        # Each tree will have its own randomly selected input features to use.\n",
        "        for _ in range(num_trees):\n",
        "            self.ensemble.append(\n",
        "                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
        "            )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        outputs = tf.zeros([batch_size, num_classes])\n",
        "\n",
        "        # Aggregate the outputs of trees in the ensemble.\n",
        "        for tree in self.ensemble:\n",
        "            outputs += tree(inputs)\n",
        "        # Divide the outputs by the ensemble size to get the average.\n",
        "        outputs /= len(self.ensemble)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2Hqxn5Uo6ft"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "batch_size = 265\n",
        "num_epochs = 10\n",
        "hidden_units = [64, 64]\n",
        "\n",
        "\n",
        "def run_experiment(model):\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    train_dataset = get_dataset_from_csv(\n",
        "        \"train.csv\", shuffle=True, batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    model.fit(train_dataset, epochs=num_epochs)\n",
        "    print(\"Model training finished\")\n",
        "\n",
        "    print(\"Evaluating the model on the test data...\")\n",
        "    test_dataset = get_dataset_from_csv(\"test.csv\", batch_size=batch_size)\n",
        "\n",
        "    _, accuracy = model.evaluate(test_dataset)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otAv-z_msOj5",
        "outputId": "7a64e064-57cd-403f-c239-1e63632688e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training the model...\n",
            "Epoch 1/10\n",
            "302/302 [==============================] - 14s 40ms/step - loss: 0.5059 - sparse_categorical_accuracy: 0.7618\n",
            "Epoch 2/10\n",
            "302/302 [==============================] - 10s 33ms/step - loss: 0.4745 - sparse_categorical_accuracy: 0.7725\n",
            "Epoch 3/10\n",
            "302/302 [==============================] - 10s 32ms/step - loss: 0.4702 - sparse_categorical_accuracy: 0.7744\n",
            "Epoch 4/10\n",
            "302/302 [==============================] - 10s 33ms/step - loss: 0.4672 - sparse_categorical_accuracy: 0.7767\n",
            "Epoch 5/10\n",
            "302/302 [==============================] - 10s 33ms/step - loss: 0.4646 - sparse_categorical_accuracy: 0.7777\n",
            "Epoch 6/10\n",
            "302/302 [==============================] - 10s 33ms/step - loss: 0.4620 - sparse_categorical_accuracy: 0.7794\n",
            "Epoch 7/10\n",
            "302/302 [==============================] - 10s 33ms/step - loss: 0.4592 - sparse_categorical_accuracy: 0.7822\n",
            "Epoch 8/10\n",
            "302/302 [==============================] - 11s 35ms/step - loss: 0.4563 - sparse_categorical_accuracy: 0.7849\n",
            "Epoch 9/10\n",
            "302/302 [==============================] - 10s 33ms/step - loss: 0.4531 - sparse_categorical_accuracy: 0.7866\n",
            "Epoch 10/10\n",
            "302/302 [==============================] - 10s 32ms/step - loss: 0.4496 - sparse_categorical_accuracy: 0.7897\n",
            "Model training finished\n",
            "Evaluating the model on the test data...\n",
            "76/76 [==============================] - 2s 20ms/step - loss: 0.5962 - sparse_categorical_accuracy: 0.6905\n",
            "Test accuracy: 69.05%\n"
          ]
        }
      ],
      "source": [
        "num_trees = 10\n",
        "depth = 10\n",
        "used_features_rate = 1.0\n",
        "num_classes = len(TARGET_LABELS)\n",
        "\n",
        "\n",
        "def create_tree_model():\n",
        "    inputs = create_model_inputs()\n",
        "    features = encode_inputs(inputs)\n",
        "    features = layers.BatchNormalization()(features)\n",
        "    num_features = features.shape[1]\n",
        "\n",
        "    tree = NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
        "\n",
        "    outputs = tree(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "tree_model = create_tree_model()\n",
        "run_experiment(tree_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKDfLzgzsOx8",
        "outputId": "7906bb08-86f5-47c8-b3b8-7f58b784a369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training the model...\n",
            "Epoch 1/10\n",
            "302/302 [==============================] - 202s 588ms/step - loss: 0.5109 - sparse_categorical_accuracy: 0.7516\n",
            "Epoch 2/10\n",
            "154/302 [==============>...............] - ETA: 1:27 - loss: 0.4773 - sparse_categorical_accuracy: 0.7702"
          ]
        }
      ],
      "source": [
        "num_trees = 25\n",
        "depth = 10\n",
        "used_features_rate = 0.5\n",
        "\n",
        "\n",
        "def create_forest_model():\n",
        "    inputs = create_model_inputs()\n",
        "    features = encode_inputs(inputs)\n",
        "    features = layers.BatchNormalization()(features)\n",
        "    num_features = features.shape[1]\n",
        "\n",
        "    forest_model = NeuralDecisionForest(\n",
        "        num_trees, depth, num_features, used_features_rate, num_classes\n",
        "    )\n",
        "\n",
        "    outputs = forest_model(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "forest_model = create_forest_model()\n",
        "\n",
        "run_experiment(forest_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "dNDFs.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}